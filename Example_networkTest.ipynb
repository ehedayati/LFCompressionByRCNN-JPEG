{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, initializers, Model, losses, optimizers\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D, Input\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2 as cv\n",
    "from pathlib import Path\n",
    "from networks import *\n",
    "from lossFunctions import *\n",
    "from helperFunctions import *\n",
    "import datetime\n",
    "lfCropSize = [375,540,7,7]\n",
    "angularDim=[7,7]\n",
    "spatialDim=[375,540]\n",
    "myDtype = tf.float32\n",
    "myIntDtype = tf.int32\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padder_cropper(spatialDim,angularDim):\n",
    "    remainder = 32 - np.array(spatialDim) % 32\n",
    "    even_odd = remainder % 2\n",
    "    padder = models.Sequential()\n",
    "    paddim = [(remainder[0]//2+even_odd[0], remainder[0]//2),(remainder[1]//2+even_odd[1], remainder[1]//2)]\n",
    "    padder.add(layers.ZeroPadding2D(padding=(paddim[0],paddim[1]),\n",
    "                                    input_shape=(spatialDim[0],spatialDim[1],3), dtype='float32'))\n",
    "\n",
    "    cropper_c = models.Sequential()\n",
    "    cropper_c.add(layers.Cropping2D(cropping=(paddim[0],paddim[1],),\n",
    "                                  input_shape=(spatialDim[0]+remainder[0],spatialDim[1]+remainder[1],3)\n",
    "                                  , dtype='float32'))\n",
    "    cropper_d = models.Sequential()\n",
    "    cropper_d.add(layers.Cropping3D(cropping=(paddim[0],paddim[1],(0,0)),\n",
    "                                  input_shape=(spatialDim[0]+remainder[0],spatialDim[1]+remainder[1],angularDim[0],angularDim[1])\n",
    "                                  , dtype='float32'))\n",
    "    lf_padder = models.Sequential()\n",
    "    lf_padder.add(layers.Reshape([spatialDim[0], spatialDim[1], angularDim[0]*angularDim[0],3], input_shape=[spatialDim[0], spatialDim[1], angularDim[0],angularDim[0],3], dtype='float32'))\n",
    "    lf_padder.add(layers.ZeroPadding3D(padding=(paddim[0],paddim[1],0), dtype='float32'))\n",
    "    lf_padder.add(layers.Reshape([spatialDim[0]+ paddim[0][0] + paddim[0][1], spatialDim[1] + paddim[1][0] + paddim[1][1], angularDim[0],angularDim[0],3], dtype='float32'))\n",
    "    return padder,cropper_c, cropper_d,lf_padder,[spatialDim[0]+remainder[0],spatialDim[1]+remainder[1]]\n",
    "\n",
    "def print_results(step,datasetLength,duration,ETA,loss,metrics,meanMetrics):\n",
    "    printable = ''\n",
    "    for key in meanMetrics.keys():\n",
    "        meanMetrics[key](metrics[key])\n",
    "    for key in meanMetrics.keys():\n",
    "        printable += key +': {:.5f} '.format(meanMetrics[key].result().numpy()) + ' '\n",
    "    print('\\r',\n",
    "        'step {:d}/{:d}: ETA: {:} stepTimeAVG: {:.5f}, Loss {:.6f}'.format(\n",
    "        step, datasetLength, str(datetime.timedelta(seconds=ETA)) ,duration.result(), loss), printable\n",
    "        , end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "@tf.function()\n",
    "def normalize_lf(lf):\n",
    "    return 2.0*(lf-0.5)\n",
    "@tf.function()\n",
    "def lfRead(name):\n",
    "    valueLF = tf.io.read_file(name)\n",
    "    lf = tf.image.decode_png(valueLF, channels=3)\n",
    "    lf = normalize_lf(tf.image.convert_image_dtype(lf,tf.float32))\n",
    "    lf = tf.transpose(tf.reshape(lf, [lfCropSize[0], lfCropSize[2], lfCropSize[1], lfCropSize[3], 3]), perm=[0, 2, 1, 3, 4])\n",
    "    return lf\n",
    "\n",
    "@tf.function()\n",
    "def centerRead(name):\n",
    "    valueLF = tf.io.read_file(name)\n",
    "    center = tf.image.decode_jpeg(valueLF, channels=3)\n",
    "    center = normalize_lf(tf.image.convert_image_dtype(center,tf.float32))\n",
    "    center.set_shape([lfCropSize[0],lfCropSize[1],3])\n",
    "    return center\n",
    "\n",
    "@tf.function()\n",
    "def read(x,y):\n",
    "    cent = centerRead(x)\n",
    "    lf = lfRead(y)\n",
    "    return cent,lf\n",
    "\n",
    "@tf.function()\n",
    "def read_center(x,y):\n",
    "    cent = centerRead(x)\n",
    "    lf = lfRead(y)\n",
    "    \n",
    "    return cent,lf[:,:,3,3,:]\n",
    "\n",
    "def input_pipeline(Names, center_filenames, lf_filenames,batch_size):\n",
    "    center_names = tf.data.Dataset.from_tensor_slices(center_filenames)\n",
    "    lf_names = tf.data.Dataset.from_tensor_slices(lf_filenames)\n",
    "    cN = tf.data.Dataset.from_tensor_slices(Names)\n",
    "    names = tf.data.Dataset.zip((center_names,lf_names))\n",
    "    ds = names.map(read)\n",
    "    return  tf.data.Dataset.zip((ds,cN)).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "def center_pipeline(center_filenames, lf_filenames,batch_size):\n",
    "    center_names = tf.data.Dataset.from_tensor_slices(center_filenames)\n",
    "    lf_names = tf.data.Dataset.from_tensor_slices(lf_filenames)\n",
    "    names = tf.data.Dataset.zip((center_names,lf_names))\n",
    "    ds = names.map(read_center).batch(batch_size, drop_remainder=True)\n",
    "    \n",
    "    return ds\n",
    "edgeP = 5\n",
    "lf_path_test = '../../../Dataset/30Scenes_7x7/LFPath/'\n",
    "center_path_test = '../../../Dataset/30Scenes_7x7/centerJpeg/'\n",
    "train_filenames_lfs_test = [os.path.join(lf_path_test, f) for f in os.listdir(lf_path_test) if f.endswith('.png')]\n",
    "train_filenames_lfs_test.sort()\n",
    "train_filenames_centers_test = [os.path.join(center_path_test, f) for f in os.listdir(center_path_test) if f.endswith('center.jpeg')]\n",
    "train_filenames_centers_test.sort()\n",
    "imName = []\n",
    "for cN in train_filenames_centers_test:\n",
    "    imName.append(Path(cN).stem)\n",
    "testDataset = input_pipeline(imName,train_filenames_centers_test,train_filenames_lfs_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "padder,centerCropper,depthCropper, lfPadder,spatial_dim = padder_cropper([375,540],angularDim)\n",
    "# myModel = LFDecompressCompress(spatial_dim,angularDim)\n",
    "myModel = LFDecompress(spatial_dim,angularDim)\n",
    "myModel.build(input_shape=[1])\n",
    "\n",
    "metrics={}\n",
    "@tf.function(experimental_compile=True)\n",
    "def step_func(center, lf_Gr):\n",
    "    cent_Gr = lf_Gr[:,:,:,3,3,:]\n",
    "    cent = padder(center)\n",
    "    lf_Gr_pad = lfPadder(lf_Gr)\n",
    "    center_refined_pad = myModel.jpegHance(cent,training=True)\n",
    "    ray_depth_pad = myModel.depthEstimation(center_refined_pad)\n",
    "    center_refined = centerCropper(center_refined_pad)\n",
    "    center_psnr, center_ssim = center_metrics(cent_Gr,center_refined)\n",
    "    metrics.update({'center_psnr':center_psnr})\n",
    "    metrics.update({'center_ssim':center_ssim})\n",
    "    \n",
    "    ray_depth = depthCropper(ray_depth_pad)\n",
    "    lf_lambertian = lambertian_lf_gen(ray_depth, center_refined)\n",
    "    lam_ssim, lam_psnr = lf_metrics(lf_Gr, lf_lambertian)\n",
    "    metrics.update({'val_ssim':lam_ssim})\n",
    "    metrics.update({'val_psnr':lam_psnr})\n",
    "    \n",
    "    dof_ssim, dof_psnr = dof_metrics(lf_Gr,lf_lambertian)    \n",
    "    metrics.update({'dof_ssim':dof_ssim})\n",
    "    metrics.update({'dof_psnr':dof_psnr})\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def validate(DS):\n",
    "    validation_time_spent_average = keras.metrics.Mean()\n",
    "    testdsLength = tf.data.experimental.cardinality(testDataset).numpy()\n",
    "    meanMetricsTest = {}\n",
    "    step = 0\n",
    "    psnrs = []\n",
    "    ssims = []\n",
    "    d_psnrs = []\n",
    "    d_ssims = []\n",
    "    for (cents,lf_gt),cNames in DS:\n",
    "        step+=1\n",
    "        start = time.time()\n",
    "        metrics = step_func(cents,lf_gt)\n",
    "        stop = time.time()\n",
    "        duration = stop - start\n",
    "        validation_time_spent_average(duration)\n",
    "        eta = int((testdsLength - step)*validation_time_spent_average.result())\n",
    "        psnrs.append(metrics['val_psnr'].numpy())\n",
    "        ssims.append(metrics['val_ssim'].numpy())\n",
    "        d_psnrs.append(metrics['dof_psnr'].numpy())\n",
    "        d_ssims.append(metrics['dof_ssim'].numpy())\n",
    "        if len(meanMetricsTest)==0:\n",
    "            for key in metrics.keys():\n",
    "                meanMetricsTest[key] = keras.metrics.Mean()\n",
    "        print_results(step,testdsLength,validation_time_spent_average,eta,0.1,metrics,meanMetricsTest)\n",
    "    return psnrs,ssims, d_psnrs, d_ssims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      " step 30/30: ETA: 0:00:00 stepTimeAVG: 0.43829, Loss 0.100000 center_psnr: 36.34637  center_ssim: 0.95564  val_ssim: 0.89885  val_psnr: 30.80236  dof_ssim: 0.95526  dof_psnr: 35.55320  \n"
     ]
    }
   ],
   "source": [
    "Allvalidate = []\n",
    "names = []\n",
    "for i in (1,):\n",
    "    print()    \n",
    "    print(i)\n",
    "    myModel.load_weights('../../weights/combinedFinalDepth/Large/Lates/2DS/ckpt_'+ str(i))\n",
    "#     myModel.load_weights('weights/combinedFinalDepth/Large/ckpt_'+str(i))\n",
    "    Allvalidate.append(validate(testDataset))\n",
    "    names.append(str(i))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
